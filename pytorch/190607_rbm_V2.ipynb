{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ising:\n",
    "    \n",
    "    # 초기화\n",
    "    def __init__(self, num_row, num_col):\n",
    "        self.num_row, self.num_col = num_row, num_col\n",
    "        self.gen_system()\n",
    "        \n",
    "    # (r x s) System configuration을 랜덤으로 생성    \n",
    "    def gen_system(self, p=0.5):\n",
    "        self.spins = torch.where(torch.rand(self.num_row, self.num_col) < p, torch.ones(1), -torch.ones(1))\n",
    "    \n",
    "    # System의 Hamiltonian를 계산\n",
    "    def hamiltonian(self):\n",
    "        H, J = 0., 1.\n",
    "        for i, j in product(range(self.num_row), range(self.num_col)):\n",
    "            for i_nhb, j_nhb in self.get_neighbors(i, j):\n",
    "                H -= J*self.spins[i_nhb, j_nhb]*self.spins[i, j]\n",
    "        return H/2\n",
    "            \n",
    "    # 격자 (i, j)의 neighbors를 리턴\n",
    "    def get_neighbors(self, i, j):\n",
    "        assert (i >= 0) and (i <= self.num_row-1) and (j >= 0) and (j <= self.num_col-1)\n",
    "        nhb = []\n",
    "        if j != self.num_col - 1: nhb.append([i, j+1])\n",
    "        if j != 0: nhb.append([i, j-1])\n",
    "        if i != self.num_row - 1: nhb.append([i+1, j])\n",
    "        if i != 0: nhb.append([i-1, j])\n",
    "        return nhb\n",
    "    \n",
    "    # n개의 데이터 생성 (Hamiltonian이 낮을수록 생성 확률 높음)\n",
    "    def gen_train_data(self, num_data):\n",
    "        self.gen_system()\n",
    "        H = self.hamiltonian()\n",
    "        data = ((1-self.spins.view(-1))/2).unsqueeze(0)\n",
    "        for _ in range(num_data-1):\n",
    "            self.gen_system()\n",
    "            H_new = self.hamiltonian()\n",
    "            p = torch.clamp(torch.exp(-(H_new-H)), max=1)\n",
    "            if torch.rand(1) < p:\n",
    "                v = ((1-self.spins.view(-1))/2).unsqueeze(0)\n",
    "                H = H_new\n",
    "            else:\n",
    "                v = data[-1].unsqueeze(0)  \n",
    "            data = torch.cat([data, v])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    \n",
    "    def __init__(self, nv, nh):\n",
    "        self.nv, self.nh = nv, nh\n",
    "        self.W = torch.randn(nh, nv)\n",
    "        self.a = torch.randn(nh)     # nh : Hidden layer 개수 (bias)\n",
    "        self.b = torch.randn(nv)     # nv : Input data 차원 (?)\n",
    "        \n",
    "    def set_train_data(self, train_data):\n",
    "        self.train_data = train_data\n",
    "        self.num_data = train_data.shape[0]\n",
    "        \n",
    "    def free_energy(self, v):\n",
    "        bv = torch.dot(self.b, v)\n",
    "        Wv = torch.mv(self.W, v)\n",
    "        F = -bv\n",
    "        for i in range(self.nh):\n",
    "            F -= torch.log(1+torch.exp(self.a[i] + Wv[i]))\n",
    "        return F\n",
    "    \n",
    "    def loglik(self):\n",
    "        F = torch.zeros(self.num_data)\n",
    "        for i in range(self.num_data):\n",
    "            F[i] = -self.free_energy(self.train_data[i])\n",
    "        ll = -F.log_softmax(0).mean()\n",
    "        return ll\n",
    "    \n",
    "    def p_v(self):\n",
    "        F = torch.zeros(self.num_data)\n",
    "        for i in range(self.num_data):\n",
    "            F[i] = -self.free_energy(self.train_data[i])\n",
    "        pv = F.softmax(0)\n",
    "        return pv\n",
    "    \n",
    "    def sigmoid_i(self, idx):\n",
    "        return torch.sigmoid(self.a + torch.mv(self.W, self.train_data[idx]))\n",
    "    \n",
    "    def gradF_i_W(self, idx):\n",
    "        grad = torch.zeros_like(self.W)\n",
    "        for i, j in product(range(self.nh), range(self.nv)):\n",
    "            grad[i, j] -= self.sigmoid_i(idx)[i] * self.train_data[idx][j]\n",
    "        return grad\n",
    "\n",
    "    def gradF_i_a(self, idx):\n",
    "        grad = torch.zeros_like(self.a)\n",
    "        for i in range(self.nh):\n",
    "            grad[i] = -self.sigmoid_i(idx)[i]\n",
    "        return grad\n",
    "\n",
    "    def gradF_i_b(self, idx):\n",
    "        grad = torch.zeros_like(self.b)\n",
    "        for i in range(self.nv):\n",
    "            grad[i] = -self.train_data[idx][i]\n",
    "        return grad\n",
    "    \n",
    "    def grad_loglik(self, param, gradient):\n",
    "        grad_loglik = torch.zeros_like(param)\n",
    "        for idx in range(self.num_data):\n",
    "            grad_loglik = (1/self.num_data - self.p_v()[idx])*gradient(idx)\n",
    "        return grad_loglik\n",
    "    \n",
    "    def update(self, learning_rate=0.01):\n",
    "        self.W -= learning_rate*self.grad_loglik(self.W, self.gradF_i_W)\n",
    "        self.a -= learning_rate*self.grad_loglik(self.a, self.gradF_i_a)\n",
    "        self.b -= learning_rate*self.grad_loglik(self.b, self.gradF_i_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_row, num_col = 3, 3\n",
    "ising = Ising(num_row, num_col)\n",
    "rbm = RBM(num_row*num_col, 2)\n",
    "rbm.set_train_data(ising.gen_train_data(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: W = tensor([[ 0.4080,  0.2312, -0.0856, -0.7043,  0.4428,  0.5958,  0.6631,  0.3955,\n",
      "         -0.6120],\n",
      "        [-1.4012,  0.5566,  0.0766, -1.8641,  0.7720, -0.6444, -0.8886,  0.3489,\n",
      "          0.3515]])\n",
      "\t a = tensor([-0.3753,  0.5275])\n",
      "\t b = tensor([-0.3838,  0.5678,  0.7601, -0.5063, -0.4074,  0.6917, -1.5545,  1.1055,\n",
      "         0.6299])\n",
      "\t loglik = 4.725319862365723\n",
      "epoch 1: W = tensor([[ 0.4080,  0.2312, -0.0852, -0.7043,  0.4428,  0.5958,  0.6631,  0.3955,\n",
      "         -0.6120],\n",
      "        [-1.4012,  0.5566,  0.0771, -1.8641,  0.7720, -0.6444, -0.8886,  0.3489,\n",
      "          0.3515]])\n",
      "\t a = tensor([-0.3750,  0.5280])\n",
      "\t b = tensor([-0.3838,  0.5678,  0.7609, -0.5063, -0.4074,  0.6917, -1.5545,  1.1055,\n",
      "         0.6299])\n",
      "\t loglik = 4.7258148193359375\n",
      "epoch 2: W = tensor([[ 0.4080,  0.2312, -0.0849, -0.7043,  0.4428,  0.5958,  0.6631,  0.3955,\n",
      "         -0.6120],\n",
      "        [-1.4012,  0.5566,  0.0776, -1.8641,  0.7720, -0.6444, -0.8886,  0.3489,\n",
      "          0.3515]])\n",
      "\t a = tensor([-0.3747,  0.5286])\n",
      "\t b = tensor([-0.3838,  0.5678,  0.7617, -0.5063, -0.4074,  0.6917, -1.5545,  1.1055,\n",
      "         0.6299])\n",
      "\t loglik = 4.726308822631836\n",
      "epoch 3: W = tensor([[ 0.4080,  0.2312, -0.0846, -0.7043,  0.4428,  0.5958,  0.6631,  0.3955,\n",
      "         -0.6120],\n",
      "        [-1.4012,  0.5566,  0.0781, -1.8641,  0.7720, -0.6444, -0.8886,  0.3489,\n",
      "          0.3515]])\n",
      "\t a = tensor([-0.3744,  0.5291])\n",
      "\t b = tensor([-0.3838,  0.5678,  0.7625, -0.5063, -0.4074,  0.6917, -1.5545,  1.1055,\n",
      "         0.6299])\n",
      "\t loglik = 4.726803779602051\n",
      "epoch 4: W = tensor([[ 0.4080,  0.2312, -0.0843, -0.7043,  0.4428,  0.5958,  0.6631,  0.3955,\n",
      "         -0.6120],\n",
      "        [-1.4012,  0.5566,  0.0787, -1.8641,  0.7720, -0.6444, -0.8886,  0.3489,\n",
      "          0.3515]])\n",
      "\t a = tensor([-0.3741,  0.5296])\n",
      "\t b = tensor([-0.3838,  0.5678,  0.7633, -0.5063, -0.4074,  0.6917, -1.5545,  1.1055,\n",
      "         0.6299])\n",
      "\t loglik = 4.727297782897949\n",
      "epoch 5: W = tensor([[ 0.4080,  0.2312, -0.0840, -0.7043,  0.4428,  0.5958,  0.6631,  0.3955,\n",
      "         -0.6120],\n",
      "        [-1.4012,  0.5566,  0.0792, -1.8641,  0.7720, -0.6444, -0.8886,  0.3489,\n",
      "          0.3515]])\n",
      "\t a = tensor([-0.3738,  0.5301])\n",
      "\t b = tensor([-0.3838,  0.5678,  0.7641, -0.5063, -0.4074,  0.6917, -1.5545,  1.1055,\n",
      "         0.6299])\n",
      "\t loglik = 4.727792739868164\n",
      "epoch 6: W = tensor([[ 0.4080,  0.2312, -0.0837, -0.7043,  0.4428,  0.5958,  0.6631,  0.3955,\n",
      "         -0.6120],\n",
      "        [-1.4012,  0.5566,  0.0797, -1.8641,  0.7720, -0.6444, -0.8886,  0.3489,\n",
      "          0.3515]])\n",
      "\t a = tensor([-0.3735,  0.5306])\n",
      "\t b = tensor([-0.3838,  0.5678,  0.7649, -0.5063, -0.4074,  0.6917, -1.5545,  1.1055,\n",
      "         0.6299])\n",
      "\t loglik = 4.728287220001221\n",
      "epoch 7: W = tensor([[ 0.4080,  0.2312, -0.0834, -0.7043,  0.4428,  0.5958,  0.6631,  0.3955,\n",
      "         -0.6120],\n",
      "        [-1.4012,  0.5566,  0.0802, -1.8641,  0.7720, -0.6444, -0.8886,  0.3489,\n",
      "          0.3515]])\n",
      "\t a = tensor([-0.3732,  0.5311])\n",
      "\t b = tensor([-0.3838,  0.5678,  0.7657, -0.5063, -0.4074,  0.6917, -1.5545,  1.1055,\n",
      "         0.6299])\n",
      "\t loglik = 4.7287821769714355\n",
      "epoch 8: W = tensor([[ 0.4080,  0.2312, -0.0831, -0.7043,  0.4428,  0.5958,  0.6631,  0.3955,\n",
      "         -0.6120],\n",
      "        [-1.4012,  0.5566,  0.0807, -1.8641,  0.7720, -0.6444, -0.8886,  0.3489,\n",
      "          0.3515]])\n",
      "\t a = tensor([-0.3728,  0.5316])\n",
      "\t b = tensor([-0.3838,  0.5678,  0.7665, -0.5063, -0.4074,  0.6917, -1.5545,  1.1055,\n",
      "         0.6299])\n",
      "\t loglik = 4.729282379150391\n",
      "epoch 9: W = tensor([[ 0.4080,  0.2312, -0.0828, -0.7043,  0.4428,  0.5958,  0.6631,  0.3955,\n",
      "         -0.6120],\n",
      "        [-1.4012,  0.5566,  0.0812, -1.8641,  0.7720, -0.6444, -0.8886,  0.3489,\n",
      "          0.3515]])\n",
      "\t a = tensor([-0.3725,  0.5322])\n",
      "\t b = tensor([-0.3838,  0.5678,  0.7673, -0.5063, -0.4074,  0.6917, -1.5545,  1.1055,\n",
      "         0.6299])\n",
      "\t loglik = 4.729777812957764\n",
      "epoch 10: W = tensor([[ 0.4080,  0.2312, -0.0825, -0.7043,  0.4428,  0.5958,  0.6631,  0.3955,\n",
      "         -0.6120],\n",
      "        [-1.4012,  0.5566,  0.0817, -1.8641,  0.7720, -0.6444, -0.8886,  0.3489,\n",
      "          0.3515]])\n",
      "\t a = tensor([-0.3722,  0.5327])\n",
      "\t b = tensor([-0.3838,  0.5678,  0.7681, -0.5063, -0.4074,  0.6917, -1.5545,  1.1055,\n",
      "         0.6299])\n",
      "\t loglik = 4.730273246765137\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 10\n",
    "for epoch in range(num_epoch+1):\n",
    "    rbm.update(0.1)\n",
    "    print('epoch {}: W = {}'.format(epoch, rbm.W))\n",
    "    print('\\t a = {}'.format(rbm.a))\n",
    "    print('\\t b = {}'.format(rbm.b))\n",
    "    print('\\t loglik = {}'.format(rbm.loglik()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
